{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is getting familiar with 'classification' by solving a natural language processing problem that is a kind of **sentimental text processing**. Here we're given some texts in which some users wrote their opinion about a movie. The sentences represent their sentiment about the movie and it says whether they like it or not. we want to process the given sentences and find out that which comment is positive and which one is has a negative opinion about that movie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three kinds of datasets are collected. They are testing, training, and validation datasets respectively. We want to build a model and then train our model (here it's a __classifier__) using _Test_ and _Training_ datasets to do the task,  labeling the _Validation_ dataset texts. After all, we will compute the accuracy of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we are using here is, IMDB dataset (sentiment analysis) in CSV format that you can download it from here: [kaggle.com](https://kaggle.com/columbine/imdb-dataset-sentiment-analysis-in-csv-format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 1, Reading the data, Fight!"
   ]
  },
  {
   "source": [
    "First things first, as i said before we have to read our datasets from a __.csv__ file that we have been downloaded before from __kaggle__ website.(actuly we have 3 datasets that we have to read)\n",
    "Python has an external library for reading some dataset formats like __csv__ and some other formats called __pandas__.(I love pandas, i mean the animal!)\n",
    "full documentations about how to use and install pandas exists on thier website, you can checkout [here](https://pandas.org) to findout how to use it and how to install it using __pip__."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Round 2, Data cleaning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first question that comes to my mind is what kind of word or letters are more important? which ones are less?\n",
    "for example, let's look at this sentence:\n",
    "\n",
    "_\" I grew up (n. 1965) watching and loving Thunderbirds, I hate them!\"_\n",
    "\n",
    "which part can represent the writer's feelings? can you say which parts are more important?\n",
    "It might be a little hard for us to say which parts are more important in a text but here, we can surely say the last part _\"i hate them!\"_ representing the exact feeling of the writer about _Thunderbirds_ he *hates them!*. In the other hand, no one will understand any feelings from some kind of Writing signs like '(', ''', '.', or even numbers like '1965'. We can omit them to have a better minimal text with fewer extra features.\n",
    "\n",
    "In otherwords we have to clean our data, and some common data cleaning methods are as bellow:\n",
    "**Common data cleaning steps:**\n",
    "* Make text all lower case\n",
    "* Remove punctuation\n",
    "* Remove numerical values\n",
    "* Remove common non-sensical text (/n)\n",
    "* Tokenize text\n",
    "* Remove stop words\n",
    "\n",
    "**More data cleaning steps after tokenization:**\n",
    "* Stemming / lemmatization\n",
    "* Parts of speech tagging\n",
    "* Create bi-grams or tri-grams\n",
    "* Deal with typos\n",
    "* And more...\n",
    "\n",
    "To do so, we have a useful library in python called __regex__ (regular expression library). (Documentations are available [here](https://docs.python.org/3/library/re.html)).\n",
    "\n",
    "In this special case our data has some more extra garbage characters inside it that we want them to be deleted, they are _html tags_, yes, in this special data sets we are using, there are some html tags that they are embeded inside the comments and we have to first delete them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "def cleanHtmlTags(text):\n",
    "    mask = re.compile(\"<.*?>\")\n",
    "    text = re.sub(mask, \"\", text)\n",
    "    return text\n",
    "\n",
    "def cleanNumbers(text):\n",
    "    mask = re.compile(\"[0-9]*\")\n",
    "    text = re.sub(mask, \"\", text)\n",
    "    return text\n",
    "\n",
    "def cleanPunctuations(text):\n",
    "    text = re.sub(\"[%s]\" % re.escape(string.punctuation), \"\", text)\n",
    "    text = re.sub(\"[‘’“”…]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}