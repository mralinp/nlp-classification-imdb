{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is getting familiar with 'classification' by solving a natural language processing problem that is a kind of **sentimental text processing**. Here we're given some texts in which some users wrote their opinion about a movie. The sentences represent their sentiment about the movie and it says whether they like it or not. we want to process the given sentences and find out that which comment is positive and which one is has a negative opinion about that movie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three kinds of datasets are collected. They are testing, training, and validation datasets respectively. We want to build a model and then train our model (here it's a __classifier__) using _Test_ and _Training_ datasets to do the task,  labeling the _Validation_ dataset texts. After all, we will compute the accuracy of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we are using here is, IMDB dataset (sentiment analysis) in CSV format that you can download it from here: [kaggle.com](https://kaggle.com/columbine/imdb-dataset-sentiment-analysis-in-csv-format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 1, Read the data, Fight!\n",
    "First things first, as i said before we have to read our datasets from a __.csv__ file that we have been downloaded before from __kaggle__ website.(actuly we have 3 datasets that we have to read)\n",
    "Python has an external library for reading some dataset formats like __csv__ and some other formats called __pandas__.(I love pandas, i mean the animal!)\n",
    "full documentations about how to use and install pandas exists on thier website, you can checkout [here](https://pandas.pydata.org/) to findout how to install and start using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                text  label\n",
       "0  I grew up (b. 1965) watching and loving the Th...      0\n",
       "1  When I put this movie in my DVD player, and sa...      0\n",
       "2  Why do people who do not know what a particula...      0\n",
       "3  Even though I have great interest in Biblical ...      0\n",
       "4  Im a die hard Dads Army fan and nothing will e...      1"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I grew up (b. 1965) watching and loving the Th...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>When I put this movie in my DVD player, and sa...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Why do people who do not know what a particula...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Even though I have great interest in Biblical ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Im a die hard Dads Army fan and nothing will e...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "db = {}\n",
    "trainDataset = pd.read_csv(\"./data-sets/Train.csv\")\n",
    "testDataset = pd.read_csv(\"./data-sets/Test.csv\")\n",
    "validDataset = pd.read_csv(\"./data-sets/Valid.csv\")\n",
    "\n",
    "trainDataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 2, Clean it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First step\n",
    "The first question that comes to my mind is what kind of words or letters are more important? which ones are less?\n",
    "for example, let's look at this sentence:\n",
    "\n",
    "_\" I grew up (n. 1965) watching and loving Thunderbirds, I hate them!\"_\n",
    "\n",
    "Which part can represent the writer's feelings? can you say which parts are more important?\n",
    "It might be a little hard for us to say which parts are more important in a text, beacuse it might depends on writers literature or phychological backgrounds which exists inside writers mind but, here we can surely say the last part _\"i hate them!\"_ representing the exact feeling of the writer about _Thunderbirds_, he *\"hates them!\"*. In the other hand, no one will understand any feelings from some kind of punctuations like '(', ''', '.', or even numbers like '1965'. We can omit them to have a better minimal text with fewer extra features.\n",
    "\n",
    "In otherwords we have to clean our data. Some common data cleaning methods are as bellow:\n",
    "\n",
    "**Common data cleaning steps:**\n",
    "* Make text all lower case\n",
    "* Remove punctuation\n",
    "* Remove numerical values\n",
    "* Remove common non-sensical text (/n)\n",
    "* Tokenize text\n",
    "* Remove stop words\n",
    "\n",
    "**More data cleaning steps after tokenization:**\n",
    "* Stemming / lemmatization\n",
    "* Parts of speech tagging\n",
    "* Create bi-grams or tri-grams\n",
    "* Deal with typos\n",
    "* And more...\n",
    "\n",
    "To do so, To do so, we can take some bits of help from a useful library in python called __regex__ (regular expression library). (Documentations are available [here](https://docs.python.org/3/library/re.html)).\n",
    "\n",
    "**Note**: In this special case, our data has some more extra garbage characters inside that We want them to be deleted. They are _html tags_ , yes, in these special data sets we are using, there are some HTML tags embedded inside the comments and firstly we have to delete them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def cleanHtmlTags(text):\n",
    "    mask = re.compile(\"<.*?>\")\n",
    "    text = re.sub(mask, \"\", text)\n",
    "    return text\n",
    "size = trainDataset.shape\n",
    "\n",
    "for i in range(size[0]):\n",
    "    txt = trainDataset[\"text\"][i]\n",
    "    txt = cleanHtmlTags(txt)\n",
    "    trainDataset[\"text\"][i] = txt\n",
    "print (\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second step\n",
    "A general paragraph may contains some numbers, dates and punctuiations. They don't give us any sentimental informations, right? let's clean them too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "def cleanNumbers(text):\n",
    "    mask = re.compile(\"[0-9]*\")\n",
    "    text = re.sub(mask, \"\", text)\n",
    "    return text\n",
    "\n",
    "def cleanPunctuations(text):\n",
    "    text = re.sub(\"[%s]\" % re.escape(string.punctuation), \"\", text)\n",
    "    text = re.sub(\"[‘’“”…]\", \"\", text)\n",
    "    return text\n",
    "    \n",
    "for i in range(size[0]):\n",
    "    txt = trainDataset[\"text\"][i]\n",
    "    txt = cleanNumbers(txt)\n",
    "    txt = cleanPunctuations(txt)\n",
    "    trainDataset[\"text\"][i] = txt\n",
    "print (\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thired step\n",
    "As said before, some words in a text can't contain any informations. In english  **stop words** are very common in sentences but they are not as informative as other parts like *verb*. A common technick that used in NLP text preprocessing is to remove this inimformative **stop word**.\n",
    "There is a python library that could be handed, **sklearn**.(Ducumentations available [here](https://scikit-learn.org/stable/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "  \n",
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "  \n",
    "stop_words = set(stopwords.words('english')) \n",
    "  \n",
    "word_tokens = word_tokenize(example_sent)\n",
    "  \n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "  \n",
    "filtered_sentence = [] \n",
    "  \n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        filtered_sentence.append(w) \n",
    "  \n",
    "print(word_tokens) \n",
    "print(filtered_sentence) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}