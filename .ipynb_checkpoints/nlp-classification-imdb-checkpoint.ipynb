{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is getting familiar with 'classification' by solving a natural language processing problem that is a kind of **sentimental text processing**. Here we're given some texts in which some users wrote their opinion about a movie. The sentences represent their sentiment about the movie and it says whether they like it or not. we want to process the given sentences and find out that which comment is positive and which one is has a negative opinion about that movie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three kinds of datasets are collected. They are testing, training, and validation datasets respectively. We want to build a model and then train our model (here it's a __classifier__) using _Test_ and _Training_ datasets to do the task,  labeling the _Validation_ dataset texts. After all, we will compute the accuracy of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we are using here is, IMDB dataset (sentiment analysis) in CSV format that you can download it from here: [kaggle.com](https://kaggle.com/columbine/imdb-dataset-sentiment-analysis-in-csv-format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 1, Read the data!\n",
    "First things first, as i said before we have to read our datasets from a __.csv__ file that we have been downloaded before from __kaggle__ website.(actuly we have 3 datasets that we have to read)\n",
    "Python has an external library for reading some dataset formats like __csv__ and some other formats called __pandas__.(I love pandas, i mean the animal!)\n",
    "full documentations about how to use and install pandas exists on thier website, you can checkout [here](https://pandas.pydata.org/) to findout how to install and start using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It's been about 14 years since Sharon Stone aw...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>someone needed to make a car payment... this i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Guidelines state that a comment must conta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This movie is a muddled mish-mash of clichés f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Before Stan Laurel became the smaller half of ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  It's been about 14 years since Sharon Stone aw...      0\n",
       "1  someone needed to make a car payment... this i...      0\n",
       "2  The Guidelines state that a comment must conta...      0\n",
       "3  This movie is a muddled mish-mash of clichés f...      0\n",
       "4  Before Stan Laurel became the smaller half of ...      0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "trainDataset = pd.read_csv(\"./data-sets/Train.csv\")\n",
    "testDataset = pd.read_csv(\"./data-sets/Test.csv\")\n",
    "validDataset = pd.read_csv(\"./data-sets/Valid.csv\")\n",
    "\n",
    "db = {'train' : trainDataset, 'test': testDataset, 'valid': validDataset}\n",
    "\n",
    "db['train'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 2, Clean it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First step\n",
    "The first question that comes to my mind is what kind of words or letters are more important? which ones are less?\n",
    "for example, let's look at this sentence:\n",
    "\n",
    "_\" I grew up (n. 1965) watching and loving Thunderbirds, I hate them!\"_\n",
    "\n",
    "Which part can represent the writer's feelings? can you say which parts are more important?\n",
    "It might be a little hard for us to say which parts are more important in a text, beacuse it might depends on writers literature or phychological backgrounds which exists inside writers mind but, here we can surely say the last part _\"i hate them!\"_ representing the exact feeling of the writer about _Thunderbirds_, he *\"hates them!\"*. In the other hand, no one will understand any feelings from some kind of punctuations like '(', ''', '.', or even numbers like '1965'. We can omit them to have a better minimal text with fewer extra features.\n",
    "\n",
    "In otherwords we have to clean our data. Some common data cleaning methods are as bellow:\n",
    "\n",
    "**Common data cleaning steps:**\n",
    "* Make text all lower case\n",
    "* Remove punctuation\n",
    "* Remove numerical values\n",
    "* Remove common non-sensical text (/n)\n",
    "* Tokenize text\n",
    "* Remove stop words\n",
    "\n",
    "**More data cleaning steps after tokenization:**\n",
    "* Stemming / lemmatization\n",
    "* Parts of speech tagging\n",
    "* Create bi-grams or tri-grams\n",
    "* Deal with typos\n",
    "* And more...\n",
    "\n",
    "To do so, To do so, we can take some bits of help from a useful library in python called __regex__ (regular expression library). (Documentations are available [here](https://docs.python.org/3/library/re.html)).\n",
    "\n",
    "**Note**: In this special case, our data has some more extra garbage characters inside that We want them to be deleted. They are _html tags_ , yes, in these special data sets we are using, there are some HTML tags embedded inside the comments and firstly we have to delete them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def cleanHtmlTags(text):\n",
    "    mask = re.compile(\"<.*?>\")\n",
    "    text = re.sub(mask, \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second step\n",
    "A general paragraph may contains some numbers, dates and punctuiations. They don't give us any sentimental informations, right? let's clean them too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanNumbers(text):\n",
    "    mask = re.compile(\"[0-9]*\")\n",
    "    text = re.sub(mask, \"\", text)\n",
    "    return text\n",
    "\n",
    "def cleanPunctuations(text):\n",
    "    text = re.sub(\"[%s]\" % re.escape(string.punctuation), \"\", text)\n",
    "    text = re.sub(\"[‘’“”…]\", \"\", text)\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thired step\n",
    "As said before, some words in a text can't contain any informations. In english  **stop words** are very common in sentences but they are not as informative as other parts like *verb*. A common technick that used in NLP text preprocessing is to remove this inimformative **stop word**.\n",
    "There is a python library that could be handed, **sklearn**.(Ducumentations available [here](https://scikit-learn.org/stable/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "example_sent = \"This is a sample sentence, (b, 1992) can't do this shit showing off the stop words filtration.\"\n",
    "\n",
    "example_sent = cleanHtmlTags(example_sent)\n",
    "example_sent = cleanNumbers(example_sent)\n",
    "example_sent = cleanPunctuations(example_sent)\n",
    "\n",
    "\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "\n",
    "  \n",
    "for name ,dataset in db.items():\n",
    "    cleaned_dataset = {'text': [], 'label': []}\n",
    "    length = dataset.shape[0]\n",
    "    for i in range(length):\n",
    "        label = dataset['label'][i]\n",
    "        text = dataset['text'][i]\n",
    "        text = cleanHtmlTags(text)\n",
    "        text = cleanNumbers(text)\n",
    "        text = cleanPunctuations(text)\n",
    "        text_tokens = word_tokenize(text)\n",
    "        filtered_text = \" \".join([w for w in text_tokens if not w in stop_words])\n",
    "        filtered_text[:-1]\n",
    "        cleaned_text = str(filtered_text[:-1])\n",
    "        cleaned_dataset['text'] += [cleaned_text]\n",
    "        cleaned_dataset['label'] += [label]\n",
    "    cleaned_dataset = pd.DataFrame(cleaned_dataset, columns = ['text', 'label']) \n",
    "    cleaned_dataset.to_csv('./data-sets/cleaned-' + name + '.csv', index = False, header=True)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting feature vectors\n",
    "\n",
    "Now we have cleaned, tokenized text but we still have some work to do!\n",
    "Lets Assume we want to extract a feature vector from this text. How many features do you think BOW will extract from it?\n",
    "\n",
    "We will introduce BOW later but now, consider that every work in the text could be a feature and to have same feature vectors for all examples we have to assume each word as an feature. How many features could we have here? For example based on our training dataset, each sample could be a 190663 dimention vector!\n",
    "it's not good at all, so we have to solve this problem and do some methods to lower our feature vector dimentions to reduce computation time.\n",
    "\n",
    "Stemming and limitization are such a good method to lower our feature vectors dimentions. now lets introduce these methods first.\n",
    "\n",
    "Let's read the data we just cleant at the previouse stage:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I grew b watching loving Thunderbirds All mate...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I put movie DVD player sat coke chips I e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why people know particular time past like feel...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Even though I great interest Biblical movies I...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Im die hard Dads Army fan nothing ever change ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  I grew b watching loving Thunderbirds All mate...      0\n",
       "1  When I put movie DVD player sat coke chips I e...      0\n",
       "2  Why people know particular time past like feel...      0\n",
       "3  Even though I great interest Biblical movies I...      0\n",
       "4  Im die hard Dads Army fan nothing ever change ...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "trainDataset = pd.read_csv(\"./data-sets/cleaned-train.csv\")\n",
    "testDataset = pd.read_csv(\"./data-sets/cleaned-test.csv\")\n",
    "validDataset = pd.read_csv(\"./data-sets/cleaned-valid.csv\")\n",
    "\n",
    "db = {'train' : trainDataset, 'test': testDataset, 'valid': validDataset}\n",
    "\n",
    "db['train'].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "for name ,dataset in db.items():\n",
    "    stemmed_dataset = {'text': [], 'label': []}\n",
    "    length = dataset.shape[0]\n",
    "    for i in range(length):\n",
    "        porterStemmer = PorterStemmer()\n",
    "        label = dataset['label'][i]\n",
    "        text = dataset['text'][i]\n",
    "        text = word_tokenize(text)\n",
    "        stemmed_text = ' '.join([porterStemmer.stem(word) for word in text])\n",
    "        stemmed_dataset['text'] += [stemmed_text]\n",
    "        stemmed_dataset['label'] += [label]\n",
    "    stemmed_dataset = pd.DataFrame(stemmed_dataset, columns = ['text', 'label']) \n",
    "    stemmed_dataset.to_csv('./data-sets/stemmed-' + name + '.csv', index = False, header=True)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing the previous rounds we achived cleaned databases and now we are after to make a feature vector from our datasets. To do so, we are able to use each methods bellow.\n",
    "\n",
    "* Bag Of Words (BOW)\n",
    "* BERT embedding\n",
    "* word2Vec\n",
    "* TF-IDF\n",
    "\n",
    "during the section we are going to use first two methods and go throw to compair our the results on two classifires, **native Bayes** and the **SVM**.\n",
    "\n",
    "> **note**: As you saw, we stored the cleaned datasets on files and, here we are going to read and use them from storage to save more time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words method\n",
    "\n",
    "description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'valid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-31f1fe9738a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mX_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'valid' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "trainDataset = pd.read_csv(\"./data-sets/stemmed-train.csv\")\n",
    "testDataset = pd.read_csv(\"./data-sets/stemmed-test.csv\")\n",
    "validDataset = pd.read_csv(\"./data-sets/stemmed-valid.csv\")\n",
    "\n",
    "db = {'train' : trainDataset, 'test': testDataset, 'valid': validDataset}\n",
    "\n",
    "db['train'].head()\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "train = pd.read_csv('./data-sets/stemmed-train.csv')\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(train['text'])\n",
    "X_valid = vectorizer.fit_transform(valid['text'])\n",
    "X_train = vectorizer.fit_transform(test['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
